{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from util import tfidf_score, textrank_info, remove_dict_val_less_than_K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('news_list.pkl', \"rb\") as f:\n",
    "    news_list = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "content_list = [news['content'] for news in news_list]\n",
    "tfidf_dict_list = tfidf_score(content_list)\n",
    "tfidf_dict = remove_dict_val_less_than_K(tfidf_dict_list[0], 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "G, vocab, tr_dict = textrank_info(content_list[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "G"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def group_by_std(tfidf_dict, lower, upper):\n",
    "    g_low = dict()\n",
    "    g_mid = dict()\n",
    "    g_hig = dict()\n",
    "\n",
    "    for kw in tfidf_dict:\n",
    "        kw_score = tfidf_dict[kw]\n",
    "        if kw_score < lower:\n",
    "            g_low[kw] = kw_score\n",
    "        elif kw_score > upper:\n",
    "            g_hig[kw] = kw_score\n",
    "        else:\n",
    "            g_mid[kw] = kw_score\n",
    "    return g_low, g_mid, g_hig\n",
    "\n",
    "def expect_boundary(g_mid):\n",
    "    g_mid_kw = [kw for kw in g_mid]\n",
    "    g_mid_tr_score = []\n",
    "    for kw in g_mid_kw:\n",
    "        g_mid_tr_score.append(tr_dict[kw])\n",
    "\n",
    "    s_min = min(g_mid_tr_score)\n",
    "    s_max = max(g_mid_tr_score)\n",
    "    return s_min, s_max\n",
    "\n",
    "def creat_expect_dict(g_low, g_hig, g_mid, s_min, s_max):\n",
    "    expect_dict = dict()\n",
    "    for kw in g_low:\n",
    "        if tr_dict[kw] > s_min:\n",
    "            expect_dict[kw] = s_min\n",
    "        else:\n",
    "            expect_dict[kw] = np.nan\n",
    "    for kw in g_hig:\n",
    "        if tr_dict[kw] < s_max:\n",
    "            expect_dict[kw] = s_max\n",
    "        else:\n",
    "            expect_dict[kw] = np.nan\n",
    "    for kw in g_mid:\n",
    "        expect_dict[kw] = np.nan\n",
    "    return expect_dict\n",
    "\n",
    "def get_expect_dict(tfidf_dict, tr_dict):\n",
    "    tfidf_dict = remove_dict_val_less_than_K(tfidf_dict)\n",
    "    std_tfidf = np.std(  [tfidf_dict[k] for k in tfidf_dict])\n",
    "    avg_tfidf = np.array([tfidf_dict[k] for k in tfidf_dict]).mean()\n",
    "    print(\"std:{:.5f},\\t avg:{:.5f}\".format(std_tfidf, avg_tfidf))\n",
    "\n",
    "    lower = avg_tfidf-(0.3 * std_tfidf)\n",
    "    upper = avg_tfidf+(0.6 * std_tfidf)\n",
    "    g_low, g_mid, g_hig = group_by_std(tfidf_dict, lower, upper)\n",
    "\n",
    "    s_min, s_max = expect_boundary(g_mid)\n",
    "    expect_dict = creat_expect_dict(g_low, g_hig, g_mid, s_min, s_max)\n",
    "    return expect_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 1. Compute the difference between expected score and current score\n",
    "# \n",
    "# There are two cases [Eq.35]\n",
    "\n",
    "def find_vocab_by_idx(vocab, idx):\n",
    "    return list(vocab.items())[nk_idx][0] # ex:('storm',31) only get the node part\n",
    "\n",
    "def calculateDifferentials_init(expect_dict, textrank_dict):\n",
    "    difference_dict = dict()\n",
    "    for node, T_j in expect_dict.items():\n",
    "        if(T_j is not np.nan):\n",
    "            A_j = textrank_dict[node]\n",
    "            d_j = T_j - A_j\n",
    "            difference_dict[node] = d_j\n",
    "        else:\n",
    "            difference_dict[node] = 0\n",
    "    return difference_dict\n",
    "\n",
    "def calculateDifferentials(G, expect_dict, difference_dict):\n",
    "    \n",
    "    norm = np.sum(G, axis=0)\n",
    "    g_norm = np.divide(G, norm, where=norm!=0) # this is ignore the 0 element in norm\n",
    "    \n",
    "    TEXTRANK_DAMPING_FACTOR = 0.85\n",
    "    is_converge = False\n",
    "    while(is_converge == False):\n",
    "        is_converge = True\n",
    "        for node, T_j in expect_dict.items():\n",
    "            if(T_j == np.nan):\n",
    "                previous_d_j = difference_dict[node]\n",
    "                d_j = 0\n",
    "                node_idx = vocab[node]\n",
    "                node_k_idxs = np.where(G[node_idx] != 0)[0]\n",
    "                for nk_idx in node_k_idxs:\n",
    "                    node_k = find_vocab_by_idx(vocab, nk_idx) \n",
    "                    d_j += difference_dict[node_k] * g_norm[node_idx][nk_idx]\n",
    "                d_j *= TEXTRANK_DAMPING_FACTOR\n",
    "                difference_dict[node] = round(d_j,6)\n",
    "                if(previous_d_j != d_j):\n",
    "                    is_converge = False\n",
    "    return difference_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ## 2. Calculate delta weight [Eq.36]\n",
    "def get_delta_weight(G, textrank_dict, difference_dict, learningRate):\n",
    "    delta_weight = np.zeros( (len(G),len(G)), dtype=float )\n",
    "    TEXTRANK_DAMPING_FACTOR = 0.85\n",
    "    for node in vocab:\n",
    "        A_i = textrank_dict[node]\n",
    "        denormalizationDenominator = 0\n",
    "        for node, out_node in G.out_edges(node):\n",
    "            denormalizationDenominator += G[node][out_node]['weight']\n",
    "#         if(node == 'NN=bifurcation'):\n",
    "#             print(textRank_score_dict[node])\n",
    "#             print(denormalizationDenominator)\n",
    "        for node, out_node in G.out_edges(node):\n",
    "            d_j = difference_dict[out_node]\n",
    "            delta_normalized_w_ij = learningRate * d_j * TEXTRANK_DAMPING_FACTOR * A_i  # [Eq.34]\n",
    "            delta_normalized_w_ij *= denormalizationDenominator\n",
    "            delta_weight_graph_dict[(node, out_node)] = delta_normalized_w_ij\n",
    "    return delta_weight_graph_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 1., ..., 0., 0., 0.],\n",
       "       [1., 0., 1., ..., 0., 0., 0.],\n",
       "       [1., 1., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 1., 1.],\n",
       "       [0., 0., 0., ..., 1., 0., 1.],\n",
       "       [0., 0., 0., ..., 1., 1., 0.]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "std:0.00682,\t avg:0.00959\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'“': -0.42651722732013786,\n",
       " 'april': -0.9505471457302341,\n",
       " 'china': -0.7008711606937543,\n",
       " 'poor': -0.05006828638012539,\n",
       " 'hualien': -0.4462784873198846,\n",
       " 'weather': -0.18797277682605318,\n",
       " '2003': -0.2700437529822368,\n",
       " 'level': -0.30360330038002936,\n",
       " 'reaching': -0.12672093667469309,\n",
       " 'hsinchu': -0.2890032791151528,\n",
       " 'eastern': -0.3129099748929841,\n",
       " 'began': -0.21306444033325656,\n",
       " 'alert': -0.43280601631488813,\n",
       " 'citing': -0.32466860722720003,\n",
       " 'residents': -0.10247016007816923,\n",
       " 'taitung': -0.3636208858800809,\n",
       " 'speed': -0.34588242399689473,\n",
       " 'chances': -0.21767409066356125,\n",
       " 'direction': -0.32740338501686883,\n",
       " 'beginning': -0.2477384999662079,\n",
       " 'peak': -0.383318812112747,\n",
       " 'yu': -0.3710249086506888,\n",
       " 'north': -0.3236713555562555,\n",
       " 'indicating': -0.33875709679409816,\n",
       " 'likely': -0.28100172203907803,\n",
       " 'county': -0.25669333759468493,\n",
       " 'wind': -0.4818824239968946,\n",
       " 'sign': -0.22972056535724006,\n",
       " 'cause': -0.33538503868141045,\n",
       " 'agency': -0.18234819833921312,\n",
       " 'month': -0.6361968328565724,\n",
       " 'year': -0.2520151638407526,\n",
       " '20': -0.2349162538366455,\n",
       " 'afternoon': -0.14418325902653728,\n",
       " 'today': -0.280534242435335,\n",
       " 'friday': -0.21249058847753755,\n",
       " 'red': -0.44922726133374336,\n",
       " 'pm': -0.10859166441197687,\n",
       " '9': -0.11355869326855861,\n",
       " '’s': -1.4077594079072628,\n",
       " 'adding': -0.8085151583795966,\n",
       " 'days': -0.47728543673707857,\n",
       " 'high': -0.5991586353586322,\n",
       " 'nation': -0.2680825298199202,\n",
       " 'early': -0.32104695000799033,\n",
       " 'added': -0.026008046922749717,\n",
       " 'time': -0.20233097860838833,\n",
       " 'center': -0.29579021239177483,\n",
       " 'surigae': 0,\n",
       " 'typhoon': 1.6362776076086085,\n",
       " 'occur': 1.580669933554924,\n",
       " 'areas': 1.401009241850618,\n",
       " 'rain': 1.5216293734102844,\n",
       " 'mountainous': 1.9572130858047383,\n",
       " 'typhoons': 1.8458681889433723,\n",
       " 'bureau': 2.218440048121135,\n",
       " 'cwb': 2.039243590785343,\n",
       " 'luzon': 1.8295693335251935,\n",
       " 'strength': 1.6904037945779748,\n",
       " 'kinmen': 1.7973287070914914,\n",
       " 'counties': 1.6971371039758307,\n",
       " 'lin': 1.9763761307576169,\n",
       " 'penghu': 1.677984820236344,\n",
       " 'southern': 1.2769056791495847,\n",
       " 'said': 0,\n",
       " 'storm': 1.6490640627587574,\n",
       " 'air': 1.676647964554018,\n",
       " 'coast': 1.8326309784688481,\n",
       " 'quality': 2.219004505839353,\n",
       " 'convective': 2.596664653819332,\n",
       " 'philippines': 2.103670639661398,\n",
       " 'peng': 2.596327744958053,\n",
       " 'island': 2.2702092730639882,\n",
       " 'weatherrisk': 2.279579404561025,\n",
       " 'ming': 2.101464641955773,\n",
       " 'southeast': 2.3249377445769612,\n",
       " 'wednesday': 2.3048992272082165,\n",
       " 'wu': 2.330078795389141,\n",
       " 'heat': 2.404541810222934,\n",
       " 'cheng': 2.101742330238593,\n",
       " 'transfer': 2.376708666362652,\n",
       " 'bring': 2.3609760284626127,\n",
       " 'generally': 2.1209920726509117,\n",
       " 'central': 0,\n",
       " 'east': 0,\n",
       " 'wrote': 0,\n",
       " 'moving': 0,\n",
       " 'strong': 0,\n",
       " 'usually': 0,\n",
       " 'taiwan': 0,\n",
       " 'yesterday': 0,\n",
       " 'km': 0,\n",
       " 'facebook': 0,\n",
       " 'gradually': 0,\n",
       " 'trough': 0,\n",
       " 'abates': 0,\n",
       " 'winds': 0,\n",
       " '2015': 0,\n",
       " 'westerly': 0,\n",
       " 'recorded': 0,\n",
       " 'remain': 0,\n",
       " 'director': 0,\n",
       " 'conditions': 0,\n",
       " 'sheng': 0,\n",
       " 'grow': 0,\n",
       " 'outlying': 0,\n",
       " 'radius': 0,\n",
       " 'prepared': 0,\n",
       " 'major': 0,\n",
       " 'good': 0,\n",
       " 'temporary': 0,\n",
       " 'maximum': 0,\n",
       " 'switch': 0,\n",
       " 'yilan': 0,\n",
       " 'landfall': 0,\n",
       " 'detected': 0,\n",
       " 'wait': 0,\n",
       " 'opportunity': 0,\n",
       " 'yi': 0,\n",
       " '吳聖宇': 0,\n",
       " 'northwest': 0,\n",
       " 'instability': 0,\n",
       " 'analyst': 0,\n",
       " 'showers': 0,\n",
       " 'northern': 0,\n",
       " 'northward': 0,\n",
       " 'southernmost': 0,\n",
       " 'away': 0,\n",
       " 'influenced': 0,\n",
       " 'channel': 0,\n",
       " 'eyewall': 0,\n",
       " 'tip': 0,\n",
       " 'executive': 0,\n",
       " 'frequently': 0,\n",
       " '林定宜': 0,\n",
       " 'approaching': 0,\n",
       " 'accelerate': 0,\n",
       " 'bashi': 0,\n",
       " 'eye': 0,\n",
       " 'unhealthy': 0,\n",
       " 'administration': 0,\n",
       " 'stagnant': 0,\n",
       " 'coasts': 0,\n",
       " 'west': 0,\n",
       " 'lightning': 0,\n",
       " 'explore': 0,\n",
       " 'disaster': 0,\n",
       " 'occurs': 0,\n",
       " 'feature': 0,\n",
       " '2': 0,\n",
       " 'forecasts': 0,\n",
       " '彭啟明': 0,\n",
       " 'stations': 0,\n",
       " '1997': 0,\n",
       " '198kph': 0,\n",
       " 'maintain': 0,\n",
       " '鵝鑾鼻': 0,\n",
       " 'occurring': 0,\n",
       " 'double': 0,\n",
       " 'september': 0,\n",
       " 'dean': 0,\n",
       " 'general': 0,\n",
       " '9kph': 0,\n",
       " 'western': 0,\n",
       " 'northeast': 0,\n",
       " 'occurred': 0,\n",
       " '鄭明典': 0,\n",
       " 'swells': 0,\n",
       " 'northerly': 0,\n",
       " 'similarly': 0,\n",
       " '1,170': 0,\n",
       " 'best': 0,\n",
       " 'july': 0,\n",
       " 'chief': 0,\n",
       " 'weaken': 0,\n",
       " 'oluanpi': 0,\n",
       " 'forecaster': 0,\n",
       " 'inc': 0,\n",
       " 'thermal': 0,\n",
       " '280': 0,\n",
       " 'environmental': 0,\n",
       " 'monitoring': 0,\n",
       " 'convection': 0,\n",
       " 'ding': 0,\n",
       " 'mountains': 0,\n",
       " 'pollutants': 0,\n",
       " 'circumfluence': 0,\n",
       " 'parts': 0,\n",
       " 'chi': 0,\n",
       " 'unfavorable': 0,\n",
       " 'protection': 0,\n",
       " 'motion': 0}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# init setting\n",
    "learningRate = 0.5\n",
    "\n",
    "\n",
    "# tfidf_dict\n",
    "content_list = [news['content'] for news in news_list]\n",
    "tfidf_dict_list = tfidf_score(content_list)\n",
    "tfidf_dict = remove_dict_val_less_than_K(tfidf_dict_list[0], 0)\n",
    "\n",
    "# textrank_info\n",
    "G, vocab, tr_dict = textrank_info(content_list[0])\n",
    "\n",
    "expect_dict = get_expect_dict(tfidf_dict, tr_dict)\n",
    "\n",
    "difference_dict = calculateDifferentials_init(expect_dict, tr_dict)\n",
    "difference_dict = calculateDifferentials(G, expect_dict, difference_dict)\n",
    "difference_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([16., 34.,  9., 16.,  6., 17., 17., 17., 14., 19., 14., 18., 24.,\n",
       "       12.,  6.,  6., 12., 14., 49., 33.,  3.,  4.,  6.,  6., 12., 10.,\n",
       "        6.,  6.,  4.,  3.,  3., 15.,  5., 12.,  6.,  6.,  6.,  5.,  4.,\n",
       "        3.,  4.,  6.,  6.,  6.,  6., 15., 12., 12.,  6., 15.,  6.,  6.,\n",
       "        6., 11.,  5.,  4.,  3.,  3.,  4.,  5.,  6.,  6.,  6.,  6., 18.,\n",
       "        4., 14.,  6.,  6., 11., 14., 12.,  6.,  5.,  4.,  3., 10., 19.,\n",
       "        6.,  5.,  3.,  4.,  5.,  6., 17., 17.,  6.,  5.,  7.,  4., 11.,\n",
       "        6.,  6.,  6.,  9.,  6.,  6., 10., 10.,  6.,  6.,  6.,  6.,  5.,\n",
       "        4.,  3.,  3.,  5.,  5.,  4.,  3.,  4.,  6.,  8.,  9.,  6.,  5.,\n",
       "        6.,  6.,  6.,  6.,  3.,  4.,  5.,  5.,  3.,  4.,  5., 12.,  6.,\n",
       "        6.,  6.,  6., 12.,  6.,  6.,  5.,  4.,  4.,  5.,  6.,  6.,  5.,\n",
       "        4.,  4.,  9.,  6.,  6.,  6.,  6.,  6.,  6.,  5.,  4.,  3.,  3.,\n",
       "        4.,  5.,  9.,  3.,  4.,  4.,  8.,  5.,  6.,  3., 14., 10.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  5.,  4.,  3.,  4.,  6.,\n",
       "        6.,  6.,  6.,  6.,  6.,  6.,  6.,  6.,  5.,  4.])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sum(G, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.array([[1,0,1],\n",
    "                 [1,0,1],\n",
    "                 [1,0,1]])\n",
    "np.sum(test,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
